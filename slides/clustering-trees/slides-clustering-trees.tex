\documentclass{beamer}

\input{../../preamble/slidespreamble.tex}

\usepackage{amsmath}
\usepackage{pdfpages}


\parskip=0.5em

%===========================================================
\title{Scientific Computing for Biologists}
\subtitle{Lecture 10: Clustering I}

\author{Instructor: Paul M. Magwene}
\date{04 November 2014}


\begin{document}

\begin{frame}
\titlepage
\end{frame}



%===========================================================
\begin{frame}
  \frametitle{Outline of Lecture}
  
\begin{itemize}
    \item Distance and dissimilarity measures
    \begin{itemize}
        \item Quantitative data
        \item Dichotomous data
        \item Qualitative data
    \end{itemize}
    \item Hierarchical clustering
    \item Neighbor-joining
    \item Minimum Spanning Tree (MST)
\end{itemize}     
  
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Similarity/Dissimilarity}

\begin{block}{Intuition}
Similarity is a measure of ``likeness'' between two entities of interest. Dissimilarity is the complement of similarity.
\end{block}


\begin{itemize}

\item Dissimilarities may be converted to similarities (and vise versa) by taking any monotonically decreasing function. For example:
\[
s = 1 - d_{ij}  \ \mbox{(for $0 \leq d_{ij} \leq 1$)}
\]

\item Dissimilarities are usually in range $0 \leq d_{ij} \leq C$ where $C$ is the maximum dissimilarity

\item Distances are one measure of dissimilarity but distances are unbounded to the right
\[
d_{ij} \in [0,\infty]
\]
\end{itemize}
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Dissimilarity Measures for Quantitative Data}


\begin{itemize}
\item Euclidean distance
\[
d_{ij} = \left\{ \sum_{k=1}^p (x_{ik} - x_{jk})^2 
         \right\} ^{1/2}
\]


\item Scaled Euclidean distance
\[
d_{ij} = \left\{ \sum_{k=1}^p w_k^2 (x_{ik} - x_{jk})^2 
         \right\} ^{1/2}
\]
where $w_k$ are suitable weight for the $k$-th variable, e.g. $\sigma_{x_k}^{-1}$ or $(max(x_k)-min(x_k))^{-1}$

\item Manhattan (taxi cab, city block) distance
\[
d_{ij} = \sum_{k=1}^p | x_{ik} - x_{jk} |         
\]

\end{itemize}
\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Dissimilarity Measures for Quantitative Data, cont.}


\begin{itemize}

\item Manhattan (taxi cab, city block) distance
\[
d_{ij} = \sum_{k=1}^p | x_{ik} - x_{jk} |         
\]

\item Chebychev distance
\[
d_{ij} = max_k \left\{ | x_{ik} - x_{jk} | \right\}
\]

\item Minkowski Metric
\[
d_{ij} = \left\{ \sum_{k=1}^p |x_{ik} - x_{jk}|^\lambda
         \right\} ^{1/ \lambda}
\]

$\lambda = 1$ is Manhattan distance, $\lambda = 2$ is Euclidean distance, $\lambda = \infty$ is Chebychev distance.

\end{itemize}
\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{More distance measures}


\begin{itemize}

\item Canberra distance (weighted Manhattan distance)
\[
d_{ij} = \sum_{k=1}^p \frac{| x_{ik} - x_{jk} |}{|x_{ik}| + |x_{jk}|}
\]

\item Cosine distance
\[
d_{ij} = \frac{x_{i\cdot} \cdot x_{j\cdot}}{|x_{i\cdot}||x_{j\cdot}|}
\]
where $x_{j\cdot}$ and $x_{j\cdot}$ indicate the row vectors, representing objects $i$ and $j$

\item Hamming distance
\[
d_{ij} = \frac{\text{count}(x_{ik} \neq x_{jk})}{p}
\]

\end{itemize}
\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Metric  Distance Functions}

A non-negative function, $g(x,y)$, is \alert{metric} if:

\begin{enumerate}

\item $g(x,y)$ satisifies the triangle inequality:
\[
g(x,y) \leq  g(x,z) + g(y,z)
\]

\item symmetric:
\[
g(x,y) = g(y,x)
\] 

\item $g(x,y) =0$ only if $x = y$



\end{enumerate}
\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Dissimilarity Measures for Dichotomous Data}

For each pair of objects (samples) of interest form a $2 \times 2$ contigency table:

\begin{center}
\begin{tabular}{l|cc}
   & 1 & 0 \\
\midrule
1 & $a$ & $b$ \\
0 & $c$ & $d$ \\
\end{tabular}
\end{center}

\begin{itemize}

\item Simple matching coefficient: 
\[
d_{ij} = 1 - \frac{a + d}{p} = \frac{b + c}{p}
\]

\item Jaccard's coefficient (ignores joint absence):
\[
d_{ij} =  \frac{b + c}{a + b + c}
\]


\item Czenkanowski coefficient: 
\[
d_{ij} = \frac{b + c}{2a + b + c}
\]


\end{itemize}
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Dissimilarity Measures for Variables}

Correlation provides a suitable measure of \emph{similarity}. Common \emph{dissimilarity} measures based on correlation include:


\begin{itemize}

\item $d_{kl} = 1 - r_{kl}$ if $r_{kl} = -1$ is taken to indicate maximum disagreement

\item $d_{kl} = 1 - r_{kl}^2$ if $r_{kl} = 1$ and $r_{kl} = -1$ are treated equivalently (predictive power)


\item Based on uncentered correlation:
\[
d_{kl} = 1 - \frac{\sum_{i=1}^n x_{ik}x_{il}} {\sum_{i=1}^n x_{ik}^2 \sum_{i=1}^n x_{il}^2}
\]

\end{itemize}
\end{frame}
%===========================================================

%===========================================================
\begin{frame}[plain,c]
\begin{center}
\Huge Introduction to Clustering
\end{center}
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Goal of Clustering}

\begin{block}{Goal}
Find ``natural groups'' in data
\end{block}

What's a ``natural group''?

\begin{itemize}
  \item Patches of high density points surrounded by patches of lower density in the $p$-dimensional space defined by the variates.
\end{itemize}

\begin{center}
\includegraphics[height=1.5in]{cluster-fig.pdf}
\end{center}

\end{frame}
%===========================================================



%===========================================================
{ 
\setbeamercolor{background canvas}{bg=} 
\includepdf[pages={10}]{lecture7-clustering.pdf}
}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Generic Algorithm for Agglomerative Hierarchical Clustering}


\begin{enumerate}
  \item Calculate a dissimilarity matrix for the $n$ items
  
  \item Join the two nearest items, $i$ and $j$

\begin{alertenv}
  \item Delete the $i$-th and $j$-th rows and columns of the dissimilarity matrix; and a new row/column that represents the dissimilarity of a new group ($i$,$j$) to all other items 
\end{alertenv}  

  \item Repeat from step 2 until there is a single group
\end{enumerate}

\begin{block}{Key Point}
 The different hierarchical clustering methods are determined by the function used to calculate the distance between groups in step 3.
\end{block}

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Single Linkage Clustering}

\begin{block}{Group Distance Measure}
  Let $i$ and $j$ be groups, and $n_i$ and $n_j$ be the number of objects in the respective groups. 

  \smallskip

  $D_{ij}$ is the \emph{smallest} of the $n_i n_j$ dissimilarities between each element of $i$ and each element of $j$
\end{block}

Properties of Single Linkage Clustering
\begin{itemize}

\item Invariant under monotonic transformation of the $d_{ij}$

\item Unaffected by ties

\item Provably nice asymptotic properties

\item Disadvantage: susceptible to chaining

\end{itemize}

\end{frame}
%===========================================================


%===========================================================
{ 
\setbeamercolor{background canvas}{bg=} 
\includepdf[pages={15-16}]{lecture7-clustering.pdf}
}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{More Hierarchical Clustering Functions}


\begin{description}

\item[Complete Linkage] -- $D_{ij}$ is the maximum of the $n_i n_j$ dissimilarities between the two groups.

\item[Group Average Methods] -- $D_{ij}$ is the average of the $n_i n_j$  dissimilarities between the two group (UPGMA, WPGMA)

\item[Centroid Method] -- $D_{ij}$ is the squared Euclidean distance between the centroids of groups $i$ and $j$

\end{description}

\end{frame}
%===========================================================
%===========================================================
%% Neighbor-joining

\begin{frame}[shrink=5]
\frametitle{Neighbor Joining}

Originally described by Saitou and Nei, 1987.

\begin{block}{Goal}
Tries to create the (unrooted) tree topology with the least branch length
(minimum-evolution criterion).
\end{block}

Basic algorithm:
\begin{enumerate}
\item Calculate matrix $Q$ (next slide) from the distance matrix
\item Find the pair of taxa in $Q$ with the lowest value; create a node on the tree that joins these two taxa (i.e. the closest neighbors)
\item Calculate the distance of each of the taxa in the pair to this new node
\item Calculate the distance of all taxa outside of this pair to the new node
\item Repeat from step 1 using the distances calculated in the previous step
\end{enumerate}

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
\frametitle{Neighbor Joining, cont.}

\[
Q_{ij} = (r - 2) d_{ij} - (R_i + R_j)
\]

where $r$ is the number of taxa, $d_{ij}$ is the distance between taxa $i$ and $j$ and $R_k$ is the row sum over row $k$ of the distance matrix ($R_k=\sum_i d_{ik}$).

\medskip

When nodes $i$ and $j$ are joined they are replaced by a node, $A$, with distance to a remaining node $k$ given by:
\[
d_{Ak} = \frac{1}{2} (d_{ik} + d_{jk} - d_{ij})
\]
    
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
\frametitle{NJ example from Saitou and Nei 1987}
\begin{center}
\includegraphics[height=3.2in]{saitou-nj-example.pdf}    
\end{center}
\end{frame}
%===========================================================



%===========================================================
\begin{frame}
  \frametitle{Minimum Spanning Tree}

\begin{block}{Goal}
Construct a tree that connects all points in the data set and whose total length is minimized.
\end{block}

\emph{Statistical applications}
\begin{itemize}
    \item highlights close neighbors in a data set
    \item useful check for distortions produced by projection techniques
    \item tests of normality
\end{itemize}
\medskip

\emph{Other applications}
\begin{itemize}
    \item urban planning/engineering
    \item circuit design
\end{itemize}

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Example Data Set}

\begin{center}
\includegraphics[height=3.1in]{points-only}
\end{center}
\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Minimum Spanning Tree: Example}

  
\begin{center}
\includegraphics[height=3.1in]{points-mst}
\end{center}
\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Relationship between MST and Single Linkage Clustering}


\begin{itemize}

\item Cut a single linkage dendrogram at height, $\delta \dashrightarrow$ clusters

\item Remove all edges in the MST with length $\geq \delta \dashrightarrow$ subgraphs corresponding to the same clusters

\end{itemize}

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{A Generic MST Algorithm}


\textbf{Input}: dissimilarity matrix, \Mtx{D}, between each object (point) of interest
\begin{enumerate}
\item Create a graph, G, where  $V = \{v_1,\ldots,v_n\}$ and $E =\{\}$ ($E$ initially empty)
\item Find the smallest dissimilarity, $d_{ij}$ where (i,j) is not in $E$.
\item Add (i,j) to $E$ if (i,j) does not create a cycle
\item Repeat from step 2 until every vertex is included in at least one edge
\end{enumerate}

Not particularly efficient algorithm, but simple. More efficient algorithms for finding MSTs include Kruskal's Algorithm and Prim's algorithm.

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Applications of the MST}

MST tends to highlight close neighbors; can be used to look for distortions associated with projections to lower dimensional spaces.

\begin{block}{Using the MST to look for Projection Distortion}
\begin{itemize}
    \item  Calculate the MST based on dissimilarity in a high-dimensional space
    \item Draw the MST edges among points in the projection space (e.g. MDS or PCA)
    \item MST edges that cross highlight geometric relationships among points that are not well represented by the projection
\end{itemize}
\end{block}
\end{frame}
%===========================================================




\end{document}



%===========================================================
\begin{frame}
  \frametitle{XXX}

\end{frame}
%===========================================================



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape" 
%%% End:
